{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDA project: predicting crowdedness in Leuven through noise and weather data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import SplineTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import shap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and adding time-related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '...'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19944\\4106023561.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"...\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\";\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\saaan\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saaan\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saaan\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saaan\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saaan\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saaan\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '...'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"...\", delimiter=\";\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines to fit all models (with cross-validation and hyperparameter tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'DateOffset' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8772\\1067384093.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;31m#Apply to df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelay_noise_weather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8772\\1067384093.py\u001b[0m in \u001b[0;36mdelay_noise_weather\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Add 6 hours to result_timestamp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mdf_noisedelay\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'result_timestamp'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDateOffset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhours\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Create key column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saaan\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__iadd__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m  11370\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iadd__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11371\u001b[0m         \u001b[1;31m# error: Unsupported left operand type for + (\"Type[NDFrame]\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 11372\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inplace_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__add__\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[operator]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  11373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11374\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__isub__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saaan\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_inplace_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m  11347\u001b[0m         \u001b[0mWrap\u001b[0m \u001b[0marithmetic\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0mto\u001b[0m \u001b[0moperate\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11348\u001b[0m         \"\"\"\n\u001b[1;32m> 11349\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  11350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11351\u001b[0m         if (\n",
      "\u001b[1;32mc:\\Users\\saaan\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\common.py\u001b[0m in \u001b[0;36mnew_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saaan\\anaconda3\\lib\\site-packages\\pandas\\core\\arraylike.py\u001b[0m in \u001b[0;36m__add__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__add__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__add__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_arith_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__radd__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saaan\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   5637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_arith_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5638\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malign_method_SERIES\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5639\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIndexOpsMixin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_arith_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saaan\\anaconda3\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   1293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1295\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marithmetic_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1297\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saaan\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36marithmetic_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[1;31m# Timedelta/Timestamp and other custom scalars are included in the check\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;31m# because numexpr will fail on it, see GH#31457\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m         \u001b[0mres_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[1;31m# TODO we should handle EAs consistently and move this check before the if/else\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saaan\\anaconda3\\lib\\site-packages\\pandas\\_libs\\tslibs\\offsets.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.offsets.BaseOffset.__add__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saaan\\anaconda3\\lib\\site-packages\\pandas\\_libs\\tslibs\\offsets.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.offsets.BaseOffset.__add__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'DateOffset' and 'str'"
     ]
    }
   ],
   "source": [
    "#pipeline to create feature matrix with delayed noise and weather data, as well as target vector.\n",
    "\n",
    "def delay_noise_weather(df):\n",
    "    # Select columns for df_noisedelay\n",
    "    df_noisedelay = df[['object_id', 'result_timestamp', 'laeq']]\n",
    "\n",
    "    # Add 6 hours to result_timestamp\n",
    "    df_noisedelay['result_timestamp'] += pd.DateOffset(hours=6)\n",
    "\n",
    "    # Create key column\n",
    "    df_noisedelay['key'] = df_noisedelay['object_id'].astype(str) + df_noisedelay['result_timestamp'].astype(str)\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df_noisedelay = df_noisedelay.drop(['laeq', 'object_id', 'result_timestamp'], axis=1)\n",
    "\n",
    "    # Select columns for df_weatherdelay\n",
    "    df_weatherdelay = df[['object_id', 'result_timestamp', 'LC_HUMIDITY', 'LC_DWPTEMP', 'LC_n', 'LC_RAD', 'LC_RAININ',\n",
    "                          'LC_DAILYRAIN', 'LC_WINDDIR', 'LC_WINDSPEED', 'LC_RAD60', 'LC_TEMP_QCL0']]\n",
    "\n",
    "    # Add 6 hours to result_timestamp\n",
    "    df_weatherdelay['result_timestamp'] += pd.DateOffset(hours=6)\n",
    "\n",
    "    # Create key column\n",
    "    df_weatherdelay['key'] = df_weatherdelay['object_id'].astype(str) + df_weatherdelay['result_timestamp'].astype(str)\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df_weatherdelay = df_weatherdelay.drop(['object_id', 'result_timestamp'], axis=1)\n",
    "\n",
    "    # Drop weather-related columns from original df\n",
    "    df = df.drop(['LC_HUMIDITY', 'LC_DWPTEMP', 'LC_n', 'LC_RAD', 'LC_RAININ',\n",
    "                  'LC_DAILYRAIN', 'LC_WINDDIR', 'LC_WINDSPEED', 'LC_RAD60', 'LC_TEMP_QCL0'], axis=1)\n",
    "\n",
    "    # Merge df with df_noisedelay and df_weatherdelay on 'key'\n",
    "    merged_df = pd.merge(df, df_noisedelay, on='key').merge(df_weatherdelay, on='key')\n",
    "\n",
    "    # Delete observations with missing values\n",
    "    merged_df = merged_df.dropna()\n",
    "\n",
    "    # Create feature matrix X and target vector y\n",
    "    X = merged_df.drop(['result_timestamp', 'laeq'], axis=1)\n",
    "    y = merged_df['laeq']\n",
    "\n",
    "    # Return feature matrix X and target vector y\n",
    "    return X, y\n",
    "\n",
    "#Apply to df\n",
    "X, y = delay_noise_weather(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create time-sensitive split for cross-validation\n",
    "ts_cv = TimeSeriesSplit(\n",
    "    n_splits=5,\n",
    "    gap=12960,\n",
    "    max_train_size=26300,\n",
    "    test_size=18347,\n",
    ")\n",
    "\n",
    "#apply split to the data\n",
    "all_splits = list(ts_cv.split(X, y))\n",
    "\n",
    "#visualize train and test sets\n",
    "#...?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spline: use ridge instead of RidgeCV because we use interactions anyway\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import SplineTransformer\n",
    "\n",
    "\n",
    "def periodic_spline_transformer(period, n_splines=None, degree=3):\n",
    "    if n_splines is None:\n",
    "        n_splines = period\n",
    "    n_knots = n_splines + 1  # periodic and include_bias is True\n",
    "    return SplineTransformer(\n",
    "        degree=degree,\n",
    "        n_knots=n_knots,\n",
    "        knots=np.linspace(0, period, n_knots).reshape(n_knots, 1),\n",
    "        extrapolation=\"periodic\",\n",
    "        include_bias=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating spline transformers\n",
    "cyclic_spline_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cyclic_month\", periodic_spline_transformer(12, n_splines=6), [\"month\"]),\n",
    "        (\"cyclic_weekday\", periodic_spline_transformer(7, n_splines=3), [\"day\"]),\n",
    "        (\"cyclic_hour\", periodic_spline_transformer(24, n_splines=12), [\"hour\"]),\n",
    "    ],\n",
    "    remainder=MinMaxScaler(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "#creating interactions\n",
    "hour_holiday_interaction = make_pipeline(\n",
    "    ColumnTransformer(\n",
    "        [\n",
    "            (\"cyclic_hour\", periodic_spline_transformer(24, n_splines=8), [\"hour\"]),\n",
    "            (\"holiday\", FunctionTransformer(lambda x: x == 1), [\"holiday\"]),\n",
    "        ]\n",
    "    ),\n",
    "    PolynomialFeatures(degree=2, interaction_only=True, include_bias=False),\n",
    ")\n",
    "\n",
    "day_holiday_interaction = make_pipeline(\n",
    "    ColumnTransformer(\n",
    "        [\n",
    "            (\"cyclic_weekday\", periodic_spline_transformer(7, n_splines=3), [\"day\"]),\n",
    "            (\"holiday\", FunctionTransformer(lambda x: x == 1), [\"holiday\"]),\n",
    "        ]\n",
    "    ),\n",
    "    PolynomialFeatures(degree=2, interaction_only=True, include_bias=False),\n",
    ")\n",
    "\n",
    "hour_weekend_interaction = make_pipeline(\n",
    "    ColumnTransformer(\n",
    "        [\n",
    "            (\"cyclic_hour\", periodic_spline_transformer(24, n_splines=8), [\"hour\"]),\n",
    "            (\"weekend\", FunctionTransformer(lambda x: x == 1), [\"weekend\"]),\n",
    "        ]\n",
    "    ),\n",
    "    PolynomialFeatures(degree=2, interaction_only=True, include_bias=False),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "#adding combining the spline pipeline and the interactions pipeline\n",
    "Ridge_splines = make_pipeline(\n",
    "    FeatureUnion(\n",
    "        [\n",
    "            (\"marginal\", cyclic_spline_transformer),\n",
    "            (\"interactions1\", hour_weekend_interaction),\n",
    "            (\"interactions2\",day_holiday_interaction ),\n",
    "            (\"interactions3\",hour_holiday_interaction),\n",
    "            \n",
    "        ]\n",
    "    ),\n",
    "    Ridge(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the code in this cell can be deleted once the cell below works\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    \"ridge__alpha\":(np.logspace(-6, 6, 25))\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(Ridge_splines, param_grid, cv=ts_cv)\n",
    "grid_search.fit(X, y)\n",
    "print(f\"The best set of parameters is: \"\n",
    "      f\"{grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pipeline to run the three models, perform hyperparameter tuning and select the best performing model\n",
    "\n",
    "def evaluate_models(X, y, cv, models):\n",
    "    best_model = None\n",
    "    best_score = float('-inf')\n",
    "\n",
    "    for model in models:\n",
    "        if isinstance(model, XGBRegressor):\n",
    "            # Parameter grid for XGBoost model\n",
    "            param_grid = {\n",
    "                'learning_rate': [0.01, 0.05, 0.1],\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [3, 4, 5]\n",
    "            }\n",
    "        elif isinstance(model, HistGradientBoostingRegressor):\n",
    "            # Parameter grid for HistGradientBoostingRegressor model\n",
    "            param_grid = {\n",
    "                'learning_rate': [0.01, 0.05, 0.1],\n",
    "                'max_iter': [100, 200, 300],\n",
    "                'max_depth': [3, 4, 5]\n",
    "            }\n",
    "        elif isinstance(model, Ridge_splines):\n",
    "            # Parameter grid for Ridge model with splines \n",
    "            param_grid = {\n",
    "                    \"ridge__alpha\":(np.logspace(-6, 6, 25))\n",
    "            }\n",
    "        else:\n",
    "            # Use default hyperparameters for other models\n",
    "            best_model = model\n",
    "            print(f\"Using default hyperparameters for model: {type(model).__name__}\")\n",
    "            continue\n",
    "\n",
    "        #Perform grid search with cross-validation\n",
    "        #add scorer: scoring = 'neg_mean_squared_error'\n",
    "        #also compute training error?=>. return_train_score=True\n",
    "        grid_search = GridSearchCV(model, param_grid=param_grid, cv=cv)\n",
    "        grid_search.fit(X, y)\n",
    "\n",
    "        # Get the best parameters and score from the search\n",
    "        best_params = grid_search.best_params_\n",
    "        best_score = grid_search.best_score_\n",
    "        print(f\"Results of Grid Search for model: {type(model).__name__}\")\n",
    "        print(f\"Best Parameters: {best_params}\")\n",
    "        print(f\"Best Score: {best_score}\")\n",
    "\n",
    "        if best_score > best_model_score:\n",
    "            best_model = model.set_params(**best_params)\n",
    "\n",
    "    if best_model is not None:\n",
    "        # Train the best model with the entire dataset\n",
    "        best_model.fit(X, y)\n",
    "        y_pred = best_model.predict(X)\n",
    "\n",
    "        mae = mean_absolute_error(y, y_pred)\n",
    "        rmse = mean_squared_error(y, y_pred, squared=False)\n",
    "\n",
    "        print(f\"Best Model: {type(best_model).__name__}\")\n",
    "        print(f\"Mean Absolute Error:     {mae:.3f}\")\n",
    "        print(f\"Root Mean Squared Error: {rmse:.3f}\")\n",
    "\n",
    "        #maybe collect cv_results of best model, so that we can use it to compare train and test errors for every fold\n",
    "\n",
    "    return best_model\n",
    "\n",
    "# Define the models to evaluate\n",
    "#not sure if I need to use the brackets... check if this works\n",
    "models = [XGBRegressor(), HistGradientBoostingRegressor(),Ridge_splines, other_model()]\n",
    "\n",
    "# Example usage with multiple models\n",
    "best_model = evaluate_models(X, y, cv, models)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of results of final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapley values\n",
    "# # replace i with index of last fold (i think tis is 4)\n",
    "X_train=X.iloc[all_splits[2][0]]\n",
    "X_test=X.iloc[all_splits[2][1]]\n",
    "y_train=y.iloc[all_splits[2][0]]\n",
    "y_test=y.iloc[all_splits[2][1]]\n",
    "#use selected model\n",
    "model=best_model\n",
    "fit = model.fit(X_train, y_train)\n",
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "#summary plot\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "#bar chart\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "#explain a single prediction: see https://www.kaggle.com/code/dansbecker/shap-values\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value, shap_values[0,:], X_test.iloc[0,:])\n",
    "# explain more predictions: don't really know how this works\n",
    "shap.force_plot(explainer.expected_value, shap_values[:500,:], X_test.iloc[:500,:])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below is a rough draft!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare predictions of different models visually\n",
    "#create pipeline to run the three models, perform hyperparameter tuning and select the best performing model\n",
    "\n",
    "def evaluate_models(X, y, cv, models):\n",
    "    #add plot\n",
    "    last_hours = slice(-96, None)\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    fig.suptitle(\"Predictions by non-linear regression models\")\n",
    "    ax.plot(\n",
    "        y.iloc[test_0].values[last_hours],\n",
    "        \"x-\",\n",
    "        alpha=0.2,\n",
    "        label=\"Actual demand\",\n",
    "        color=\"black\",\n",
    "    )\n",
    "    best_model = None\n",
    "    best_score = float('-inf')\n",
    "\n",
    "    for model in models:\n",
    "        if isinstance(model, XGBRegressor):\n",
    "            # Parameter grid for XGBoost model\n",
    "            param_grid = {\n",
    "                'learning_rate': [0.01, 0.05, 0.1],\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [3, 4, 5]\n",
    "            }\n",
    "        elif isinstance(model, HistGradientBoostingRegressor):\n",
    "            # Parameter grid for HistGradientBoostingRegressor model\n",
    "            param_grid = {\n",
    "                'learning_rate': [0.01, 0.05, 0.1],\n",
    "                'max_iter': [100, 200, 300],\n",
    "                'max_depth': [3, 4, 5]\n",
    "            }\n",
    "        else:\n",
    "            # Use default hyperparameters for other models\n",
    "            best_model = model\n",
    "            print(f\"Using default hyperparameters for model: {type(model).__name__}\")\n",
    "            continue\n",
    "\n",
    "        #Perform grid search with cross-validation\n",
    "        #add scorer: scoring = 'neg_mean_squared_error'\n",
    "        #also compute training error?=>. return_train_score=True\n",
    "        grid_search = GridSearchCV(model, param_grid=param_grid, cv=cv)\n",
    "        grid_search.fit(X, y)\n",
    "        #use the best parameters of the model to make a prediction\n",
    "        model_name=grid_search.best_estimator_\n",
    "        model_name.fit(X.iloc[train_0], y.iloc[train_0])\n",
    "        #make sure the name of the predictions is that of the model in models\n",
    "        model_predict=model_name.predict(X.iloc[test_0])\n",
    "        #plot predictions\n",
    "        ax.plot(\n",
    "        model_predict[last_hours],\n",
    "        \"x-\",\n",
    "        label=model,\n",
    "    )\n",
    "\n",
    "\n",
    "        # Get the best parameters and score from the search\n",
    "        best_params = grid_search.best_params_\n",
    "        best_score = grid_search.best_score_\n",
    "        print(f\"Results of Grid Search for model: {type(model).__name__}\")\n",
    "        print(f\"Best Parameters: {best_params}\")\n",
    "        print(f\"Best Score: {best_score}\")\n",
    "\n",
    "        if best_score > best_model_score:\n",
    "            best_model = model.set_params(**best_params)\n",
    "\n",
    "    if best_model is not None:\n",
    "        # Train the best model with the entire dataset\n",
    "        best_model.fit(X, y)\n",
    "        y_pred = best_model.predict(X)\n",
    "\n",
    "        mae = mean_absolute_error(y, y_pred)\n",
    "        rmse = mean_squared_error(y, y_pred, squared=False)\n",
    "\n",
    "        print(f\"Best Model: {type(best_model).__name__}\")\n",
    "        print(f\"Mean Absolute Error:     {mae:.3f}\")\n",
    "        print(f\"Root Mean Squared Error: {rmse:.3f}\")\n",
    "\n",
    "        #maybe collect cv_results of best model, so that we can use it to compare train and test errors for every fold\n",
    "\n",
    "    return best_model\n",
    "\n",
    "\n",
    "# Define the models to evaluate\n",
    "models = [XGBRegressor(), HistGradientBoostingRegressor(), other_model()]\n",
    "\n",
    "# Example usage with multiple models\n",
    "best_model = evaluate_models(X, y, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course_mda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
