{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDA project: predicting crowdedness in Leuven through noise and weather data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import SplineTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and adding time-related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"...\", delimiter=\";\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines to fit all models (with cross-validation and hyperparameter tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline to create feature matrix with delayed noise and weather data, as well as target vector.\n",
    "\n",
    "def delay_noise_weather(df):\n",
    "    # Select columns for df_noisedelay\n",
    "    df_noisedelay = df[['object_id', 'result_timestamp', 'laeq']]\n",
    "\n",
    "    # Add 6 hours to result_timestamp\n",
    "    df_noisedelay['result_timestamp'] += pd.DateOffset(hours=6)\n",
    "\n",
    "    # Create key column\n",
    "    df_noisedelay['key'] = df_noisedelay['object_id'].astype(str) + df_noisedelay['result_timestamp'].astype(str)\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df_noisedelay = df_noisedelay.drop(['laeq', 'object_id', 'result_timestamp'], axis=1)\n",
    "\n",
    "    # Select columns for df_weatherdelay\n",
    "    df_weatherdelay = df[['object_id', 'result_timestamp', 'LC_HUMIDITY', 'LC_DWPTEMP', 'LC_n', 'LC_RAD', 'LC_RAININ',\n",
    "                          'LC_DAILYRAIN', 'LC_WINDDIR', 'LC_WINDSPEED', 'LC_RAD60', 'LC_TEMP_QCL0']]\n",
    "\n",
    "    # Add 6 hours to result_timestamp\n",
    "    df_weatherdelay['result_timestamp'] += pd.DateOffset(hours=6)\n",
    "\n",
    "    # Create key column\n",
    "    df_weatherdelay['key'] = df_weatherdelay['object_id'].astype(str) + df_weatherdelay['result_timestamp'].astype(str)\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df_weatherdelay = df_weatherdelay.drop(['object_id', 'result_timestamp'], axis=1)\n",
    "\n",
    "    # Drop weather-related columns from original df\n",
    "    df = df.drop(['LC_HUMIDITY', 'LC_DWPTEMP', 'LC_n', 'LC_RAD', 'LC_RAININ',\n",
    "                  'LC_DAILYRAIN', 'LC_WINDDIR', 'LC_WINDSPEED', 'LC_RAD60', 'LC_TEMP_QCL0'], axis=1)\n",
    "\n",
    "    # Merge df with df_noisedelay and df_weatherdelay on 'key'\n",
    "    merged_df = pd.merge(df, df_noisedelay, on='key').merge(df_weatherdelay, on='key')\n",
    "\n",
    "    # Delete observations with missing values\n",
    "    merged_df = merged_df.dropna()\n",
    "\n",
    "    # Create feature matrix X and target vector y\n",
    "    X = merged_df.drop(['result_timestamp', 'laeq'], axis=1)\n",
    "    y = merged_df['laeq']\n",
    "\n",
    "    # Return feature matrix X and target vector y\n",
    "    return X, y\n",
    "\n",
    "#Apply to df\n",
    "X, y = delay_noise_weather(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create time-sensitive split for cross-validation\n",
    "ts_cv = TimeSeriesSplit(\n",
    "    n_splits=5,\n",
    "    gap=12960,\n",
    "    max_train_size=26300,\n",
    "    test_size=18347,\n",
    ")\n",
    "\n",
    "#apply split to the data\n",
    "all_splits = list(ts_cv.split(X, y))\n",
    "\n",
    "#visualize train and test sets\n",
    "...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pipeline to run the three models, perform hyperparameter tuning and select the best performing model\n",
    "\n",
    "def evaluate_models(X, y, cv, models):\n",
    "    best_model = None\n",
    "    best_score = float('-inf')\n",
    "\n",
    "    for model in models:\n",
    "        if isinstance(model, XGBRegressor):\n",
    "            # Parameter grid for XGBoost model\n",
    "            param_grid = {\n",
    "                'learning_rate': [0.01, 0.05, 0.1],\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [3, 4, 5]\n",
    "            }\n",
    "        elif isinstance(model, HistGradientBoostingRegressor):\n",
    "            # Parameter grid for HistGradientBoostingRegressor model\n",
    "            param_grid = {\n",
    "                'learning_rate': [0.01, 0.05, 0.1],\n",
    "                'max_iter': [100, 200, 300],\n",
    "                'max_depth': [3, 4, 5]\n",
    "            }\n",
    "        else:\n",
    "            # Use default hyperparameters for other models\n",
    "            best_model = model\n",
    "            print(f\"Using default hyperparameters for model: {type(model).__name__}\")\n",
    "            continue\n",
    "\n",
    "        # Perform grid search with cross-validation\n",
    "        grid_search = GridSearchCV(model, param_grid=param_grid, cv=cv)\n",
    "        grid_search.fit(X, y)\n",
    "\n",
    "        # Get the best parameters and score from the search\n",
    "        best_params = grid_search.best_params_\n",
    "        best_score = grid_search.best_score_\n",
    "        print(f\"Results of Grid Search for model: {type(model).__name__}\")\n",
    "        print(f\"Best Parameters: {best_params}\")\n",
    "        print(f\"Best Score: {best_score}\")\n",
    "\n",
    "        if best_score > best_model_score:\n",
    "            best_model = model.set_params(**best_params)\n",
    "\n",
    "    if best_model is not None:\n",
    "        # Train the best model with the entire dataset\n",
    "        best_model.fit(X, y)\n",
    "        y_pred = best_model.predict(X)\n",
    "\n",
    "        mae = mean_absolute_error(y, y_pred)\n",
    "        rmse = mean_squared_error(y, y_pred, squared=False)\n",
    "\n",
    "        print(f\"Best Model: {type(best_model).__name__}\")\n",
    "        print(f\"Mean Absolute Error:     {mae:.3f}\")\n",
    "        print(f\"Root Mean Squared Error: {rmse:.3f}\")\n",
    "\n",
    "    return best_model\n",
    "\n",
    "\n",
    "# Define the models to evaluate\n",
    "models = [XGBRegressor(), HistGradientBoostingRegressor(), other_model()]\n",
    "\n",
    "# Example usage with multiple models\n",
    "best_model = evaluate_models(X, y, cv, models)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of results of final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course_mda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
